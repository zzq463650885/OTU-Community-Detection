# Records of experiments
                        
# Ideas:
    增加重构边的线性权重中间 8*8 矩阵，而非直接向量相乘; 优化内积计算，使用特定的内积公式;
    损失函数优化（谨慎增加新损失函数）
    前向传播公式升级为注意力机制
    使用网格调参法    
    GCN切比雪夫近似公式微调  lamda_max change
特征或许可以直接用bio72而不是dpwk数据。(bad)
louvain 算是目前市面上提到的和使用过的最常用的社区发现算法之一了，除此之外就是     infomap
增加对比实验方法和最新方法

中期工作创新点： 调整GCN运算、自编码器恢复features(0.7328)(与diag（sum/100）一起0.7291)

# 改修正后邻接矩阵对角线、LeakyReLU
    # adj.setdiag(1) # 0.7274
    # adj.setdiag(0.5) # 0.7252, about 71
    # adj.setdiag(2) # bad 0.64
    # adj.setdiag(10) # bad dpwk 0.65
    # adj.setdiag(adj.sum(1)) # 一般 dpwk 0.70  17: 0.6963
    adj.setdiag(adj.sum(1)/100) # low loss 0.216 but low modul 0.717,0.726  ``` 0.7343 ``` 
    # adj.setdiag(0.5+adj.sum(1)/200) # 0.7173
    # adj.setdiag(1+adj.sum(1)/100) #  0.7301 0.7167 0.7003



# Total Scores         
    method     EdMot     LabelProp    SCD       GEMSEC    Spectral   Louvain       NOCD      OTUCD
    time       77 min    5 min        45 min    4h        3 min      2min         
    meta72     0.0182    0.5332       0.3128    0.0531    0.4100     0.5292        0.5371         
    dpwk(0.5)  0.0035    0.7057       0.5534    0.4448    0.6568     0.6798        0.7274     0.7315
    n2v(0.8)   0         0.7260       0.5713    0.4275    0.6546     0.7176        0.7267     0.7479
    lle(0.5)   0         0.6881       0.6969    0.3922    0.3614     0.8092        0.7447
    line(0.7)  0         0.7169       0.6928    0.5468    0.7289     0.7812        0.7943
# Notes
    Louvain to pearson、kneighbour、merged: good totally

# Transverse to draw    
                        meta72        dpwk      n2v       lle        line
    LabelProp           0.5332        0.7057    0.7260    0.6881     0.7169
    SCD                 0.3128        0.5534    0.5713    0.6969     0.6928
    GEMSEC              0.0531        0.4448    0.4275    0.3922     0.5468    
    Spectral            0.4100        0.6568    0.6546    0.3614     0.7289     
    Louvain             0.5292        0.6798    0.7176    0.8092     0.7812                   
    OTUCD               0.5371        0.7274    0.7267    0.7447     0.7943


# Specific Nocd Scores 
    ( default parameters: l2_reg:1e-2 lr:1e-3 dropout:0.5 epochs:1000 batch:20000 )       
    method     nocd     []      [32]     [64]     [128]     [256]       [512]       [1024]    [2048]    [4096]     
    time                2.5min  4min     5min     5min      10min       16min       31min     51min     3h
    bio72      0.5371   bad                       0.4930    0.5330      0.5371      0.5007    0.5345    0.5121
    dpwk(0.5)  0.7260   bad     0.6867   0.6731   0.7071    0.6832      0.7003      0.6902    0.6954    0.6858
    n2v(0.8)   0.7267   bad     0.7029   0.6852   0.7226    0.7214      0.7189      0.7236    0.7229    0.7196  
    lle(0.5)   0.7347   bad     0.7208   0.7231   0.7237    0.7253      0.7270      0.7248    0.7344    0.7200
    line(0.7)  0.7943   bad     0.7801   0.7913   0.7826    0.7618      0.7799      0.7943    0.7929    0.7848

# Specific Nocd Scores 
    ( default parameters: l2_reg:1e-2 lr:1e-3 dropout:0.5 epochs:1000 batch:20000 )        
    method     nocd   X_bio[512]  X_rand[512]   Imp_GCN[512]    [512,1024]    [512,512,1024]  [512,1024,1024,512] 
    time                          25min         1min            61min         107min          2h
    bio72                                                       0.5228        0.5252        
    dpwk(0.5)         0.6753      bad           0.6079          0.7064        0.7018          0.7083
    n2v(0.8)          0.6866      bad           0.6615          0.6984        0.6991          0.7267
    lle(0.5)          0.7238      bad           0.6477          0.7330        0.7347          0.7447
    line(0.7)         0.7750      bad           0.7379          0.7655        0.7671          0.7904

# dpwk (one varible parameters, others default fixed)
# n_clusters  [2, 3, 4, 5, 6, 7, 8] [9,10,11,12] [13,14,15] [17,18,19,20,21] [22, 23, 24, 25]
    [0.4022, 0.5417, 0.6139, 0.6439, 0.6722, 0.6829, 0.7070] [0.6798,0.6932,0.7228,0.7075] [:12]    
    [0.7193, 0.7104, 0.7150]  ``` 17:[0.7274 ``` ,  0.7208,0.7138,0.7260,0.7125]  [0.7231, 0.7260, 0.7130, 0.7222] 
    [26, 27, 28, 29, 30, 31]    [16,32,64,128] 
    [0.7235, 0.7260, 0.7231, 0.7216, 0.7219, 0.7149] [0.7250,0.7166,0.7116,0.6672]
    [33 34 35 36]   [37 38 39 40]   [41 42 43 44]
    [0.7106, 0.7062, 0.7153, 0.6992]  [0.7197, 0.7140, 0.7166, 0.7140]  [0.7214, 0.7038, 0.7181, 0.7156]  
    [45 46 47 48 49]    [50 51 52 53 54]
    [0.7148, 0.7102, 0.7212, 0.7179, 0.7150]    [0.7147, 0.7198, 0.7127, 0.7155, 0.7146]
    [55 56 57 58 59]    [60 61 62 63 64]
    [0.7147, 0.7154, 0.7138, 0.7181, 0.7173]    [0.7127, 0.7226, 0.7128, 0.7221, 0.7174]
    
# hidden layer [32]     [64]     [128]     [256]       [512]       [1024]    [2048]    [4096]
                0.6867   0.6731   0.7071    0.6832      0.7003      0.6902    0.6954    0.6858
# bad draw batch_size [10000, 20000, 30000, 40000]
    [0.6977, 0.7013, 0.6884, 0.6976]
# [balance_loss, stochastic_loss] [[True, True], [True, False], [False, True], [False, False]]
    [0.6966, 0.6940, 0.6856, 0.6688]
# weight_decay_list = [1e-1,1e-2,1e-3,1e-4]
    [0.6994, 0.6893, 0.6841, 0.6946]
# dropout_list = [0.1, ..., 0.9]
    0.7079, 0.6899, 0.6964, 0.6905, 0.7003, 0.6993, 0.7103, 0.6988, 0.6837
# lr_list = [1e-1,1e-2,1e-3,1e-4]
    [0.5260, 0.6825, 0.6948, 0.6721]
# batch_norm_list [True, False]
    [0.6987434024568999, -0.00428082510617247]

# hidden_sizes  [128, 128]  [128, 256]  [128, 512]  [128, 1024]  [256,128]  [256,256]  [256,512]
    0.6926 0.6804 0.7049 0.6944 0.6900 0.7000 0.6929 0.7036
    [[512, 128], [512, 256], [512, 512], [512, 1024], [1024, 128], [1024, 256], [1024, 512], [1024, 1024]]
    [ 0.6836, 0.6972, 0.7045, 0.7077, 0.7012, 0.6937, 0.6970, 0.6976]
# 3层hidden 
# 128    [[128, 128, 128], [128, 128, 256], [128, 128, 512], [128, 128, 1024]]
    [ ``` 0.7104 ``` , 0.6752, 0.7074, 0.7051]
    [[128, 256, 128], [128, 256, 256], [128, 256, 512], [128, 256, 1024]]
    [0.7056, 0.7027, 0.6888, 0.6904]
    [[128, 512, 128], [128, 512, 256], [128, 512, 512], [128, 512, 1024]]
    [0.6885, 0.6932, 0.7046, 0.7045]
    [[128, 1024, 128], [128, 1024, 256], [128, 1024, 512], [128, 1024, 1024]]
    [0.6939, 0.6948, 0.6876, 0.6973]
# 256    [[256, 128, 128], [256, 128, 256], [256, 128, 512], [256, 128, 1024]]
    [0.7033, 0.7017, 0.6882, 0.6946]
    [[256, 256, 128], [256, 256, 256], [256, 256, 512], [256, 256, 1024]]
    [0.7099, 0.6702, 0.6897, 0.6967]
    [[256, 512, 128], [256, 512, 256], [256, 512, 512], [256, 512, 1024]]
    [0.7018,   ``` 0.7103 ```   , 0.6945, 0.6963]
    [[256, 1024, 128], [256, 1024, 256], [256, 1024, 512], [256, 1024, 1024]]
    [ ``` 0.7105 ``` , 0.7000, 0.6975, 0.7034]
# 512   [[512, 128, 128], [512, 128, 256], [512, 128, 512], [512, 128, 1024]]
    [0.6979, 0.6958, 0.7007, 0.7045]
    [[512, 256, 128], [512, 256, 256], [512, 256, 512], [512, 256, 1024]]
    [0.6950, 0.7084,  ``` 0.7113 ``` , 0.6937]
    [[512, 512, 128], [512, 512, 256], [512, 512, 512], [512, 512, 1024]]
    [0.7039, 0.7024, 0.7045, ``` 0.7103 ``` ]
    [[512, 1024, 128], [512, 1024, 256], [512, 1024, 512], [512, 1024, 1024]]
    [0.7067, 0.7065, 0.6940, 0.7006]
# 1024    [[1024, 128, 128], [1024, 128, 256], [1024, 128, 512], [1024, 128, 1024]]
    [0.7008,  ``` 0.7108 ``` , 0.7085, 0.7100]
    [[1024, 256, 128], [1024, 256, 256], [1024, 256, 512], [1024, 256, 1024]]
    [0.6973, 0.6976, 0.7046, 0.7015]
    [[1024, 512, 128], [1024, 512, 256], [1024, 512, 512], [1024, 512, 1024]]
    [0.6990, 0.7051, 0.7069, 0.6986]
    [[1024, 1024, 128], [1024, 1024, 256], [1024, 1024, 512], [1024, 1024, 1024]]
    [0.6978, 0.6983, 0.7049, 0.7032]
# 4 hidden layers
    [[128, 128, 128, 128]]
    [0.7043584445950789]

# NOCD  17 [256,1024,256] 0.7 dropout 0.6190.
        17 [512,1024] 0.7 dropout     0.7210


# Notes of comparation Experiments
# louvain: 50, 30, 17, 8 to bio72 too low;
# label prop: 5 to dpwk ok; 4 to n2v ok; 3 to lle 0.6881; 4 to line 0.8075
3 to n2v  0.6118; 3 to line 0.7169