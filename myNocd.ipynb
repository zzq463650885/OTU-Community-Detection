{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nocd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "from components.utils import load_edgelist_graph,load_ordered_adjlist_graph\n",
    "from components.evaluation import modularity, symmetric_matrix_modularity\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、Defining and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset\n",
    " - `A` (adjacency matrix) is a `scipy.sparse.csr_matrix` of size `[N, N]`\n",
    " - `X` (attribute matrix) is a `np.ndarray` of size `[N, D]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2852600/2852600 [00:21<00:00, 130904.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G nodes 25023,edges 2877623 with self-loops read already\n",
      "x shape:25023 * 128\n"
     ]
    }
   ],
   "source": [
    "name = 'dpwk'\n",
    "graph_type = 'merged'\n",
    "\n",
    "graph_adjlist_path  =  './graphs_' + graph_type + '/' + name + '.adjlist'\n",
    "graph_edgelist_path =  './graphs_' + graph_type + '/' + name + '.edgelist'\n",
    "# X directly bio72\n",
    "X_path = './features/' + name + '.txt'\n",
    "X = np.loadtxt( X_path, dtype=float)\n",
    "G_nx = load_ordered_adjlist_graph( graph_adjlist_path )\n",
    "A = load_edgelist_graph(graph_edgelist_path)\n",
    "N, D = X.shape\n",
    "print('x shape:{} * {}'.format(N, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 8\n",
    "hidden_sizes = [512]                  # hidden sizes of the GNN\n",
    "weight_decay = 1e-2                   # strength of L2 regularization on GNN weights\n",
    "dropout = 0.5                         # whether to use dropout\n",
    "batch_norm = True                     # whether to use batch norm\n",
    "lr = 1e-3                             # learning rate\n",
    "max_epochs = 1000                     # number of epochs to train\n",
    "display_step = 25                     # how often to compute validation loss\n",
    "balance_loss = True                   # whether to use balanced loss\n",
    "stochastic_loss = True                # whether to use stochastic or full-batch training\n",
    "batch_size = 20000                    # batch size (only for stochastic training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select & normalize the feature matrix\n",
    "\n",
    "For some datasets where the features are very informative / correlated with the community structure it's better to use `X` as input (e.g. co-authorship networks w/ keywords as node features). Otherwise, you should try using `A` or `[A, X]` as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn_model_train_eval( idx:int):\n",
    "    \n",
    "    '''get customized parameters'''\n",
    "    n_clusters = output_dims[idx]\n",
    "    \n",
    "    \"\"\"x tensor data\"\"\"\n",
    "    x_norm = normalize(X)  # node features\n",
    "    # x_norm = normalize(A)  # adjacency matrix\n",
    "    # x_norm = sp.hstack([normalize(X), normalize(A)])  # concatenate A and X\n",
    "    x_norm = torch.Tensor(x_norm).cuda().to(torch.float32)\n",
    "    \"\"\"gnn model\"\"\"\n",
    "    sampler = nocd.sampler.get_edge_sampler(A, batch_size, batch_size, num_workers=5)\n",
    "    gnn = nocd.nn.GCN(x_norm.shape[1], hidden_sizes, n_clusters, batch_norm=batch_norm, dropout=dropout).cuda()\n",
    "    # gnn = nocd.nn.ImprovedGCN(x_norm.shape[1], hidden_sizes, n_clusters, dropout=dropout).cuda()\n",
    "    adj_norm = gnn.normalize_adj(A)\n",
    "    decoder = nocd.nn.BerpoDecoder(N, A.nnz, balance_loss=balance_loss)\n",
    "    opt = torch.optim.Adam(gnn.parameters(), lr=lr)\n",
    "    \"\"\"train\"\"\"\n",
    "    val_loss = np.inf\n",
    "    validation_fn = lambda: val_loss\n",
    "    early_stopping = nocd.train.NoImprovementStopping(validation_fn, patience=10)\n",
    "    model_saver = nocd.train.ModelSaver(gnn)\n",
    "\n",
    "    x_display_epochs, val_full_loss_list, x_train_epochs, train_loss_list = [],[],[],[]\n",
    "    it = iter( sampler )\n",
    "    # for epoch in tqdm( range( max_epochs+1 ) ):\n",
    "    for epoch in range( max_epochs+1 ) :\n",
    "        batch = next(it)\n",
    "        if epoch % display_step == 0:\n",
    "            with torch.no_grad():\n",
    "                gnn.eval()\n",
    "                # Compute validation loss\n",
    "                Z = F.relu(gnn(x_norm, adj_norm))\n",
    "                val_loss = decoder.loss_full(Z, A)\n",
    "                x_display_epochs.append( epoch )\n",
    "                val_full_loss_list.append( val_loss )\n",
    "                # print(f'Epoch {epoch:4d}, loss.full = {val_loss:.4f} ')\n",
    "\n",
    "                # Check if it's time for early stopping / to save the model\n",
    "                early_stopping.next_step()\n",
    "                if early_stopping.should_save():\n",
    "                    model_saver.save()\n",
    "                if early_stopping.should_stop():\n",
    "                    stop_epoch = max_epochs\n",
    "                    print(f'Breaking due to early stopping at epoch {epoch}')\n",
    "                    break\n",
    "\n",
    "        # Training step\n",
    "        gnn.train()\n",
    "        opt.zero_grad()\n",
    "        Z = F.relu(gnn(x_norm, adj_norm))\n",
    "        ones_idx, zeros_idx = batch\n",
    "        if stochastic_loss:\n",
    "            loss = decoder.loss_batch(Z, ones_idx, zeros_idx)\n",
    "        else:\n",
    "            loss = decoder.loss_full(Z, A)\n",
    "        loss += nocd.utils.l2_reg_loss(gnn, scale=weight_decay)\n",
    "        x_train_epochs.append( epoch )\n",
    "        train_loss_list.append( loss )\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    '''evaluate'''\n",
    "    Z_pred = F.relu(gnn(x_norm, adj_norm))\n",
    "    # Z_pred = Z.cpu().detach().numpy() > thresh \n",
    "    Z_pred = np.argmax(Z_pred.cpu().detach().numpy(), 1)\n",
    "    model_saver.restore()\n",
    "    score = symmetric_matrix_modularity(G_nx, Z_pred)\n",
    "    # print(f'\\nFinal symmetric_matrix_modularity = {:.8f}'.format(score))  \n",
    "    \n",
    "    '''record'''\n",
    "    epoch_results.append(x_train_epochs[-1])\n",
    "    full_loss_results.append(val_full_loss_list[-1])\n",
    "    eval_results.append(score)\n",
    "    '''free'''\n",
    "    torch.cuda.empty_cache()\n",
    "    '''show parameters and results'''\n",
    "    print('index:{}'.format(idx),end='\\t')\n",
    "    print('output_dim:{}'.format(output_dims[idx]),end='\\t')\n",
    "    print('epoch_result:{}'.format(epoch_results[idx]),end='\\t')\n",
    "    print('full_loss_results:{}'.format(full_loss_results[idx]),end='\\t')\n",
    "    print('eval_results:{}'.format(eval_results[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [14:15<42:47, 855.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:0\toutput_dim:18\tepoch_result:1000\tfull_loss_results:0.21708233654499054\teval_results:0.7208499347578388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [28:19<28:17, 848.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:1\toutput_dim:19\tepoch_result:1000\tfull_loss_results:0.21665915846824646\teval_results:0.7138786046235402\n",
      "Breaking due to early stopping at epoch 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [41:21<13:38, 818.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:2\toutput_dim:20\tepoch_result:899\tfull_loss_results:0.20927338302135468\teval_results:0.7260651991530943\n",
      "Breaking due to early stopping at epoch 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [54:30<00:00, 817.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:3\toutput_dim:21\tepoch_result:899\tfull_loss_results:0.20924727618694305\teval_results:0.7125074419948373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''customized global parameters'''\n",
    "output_dims = [18,19,20,21] # ,13,14,15,16,17,18,19,20] # ,16,32,64,128\n",
    "\n",
    "'''global result records'''\n",
    "epoch_results = []\n",
    "full_loss_results = []\n",
    "eval_results = []\n",
    "\n",
    "'''work'''\n",
    "for idx in tqdm(range(len(output_dims))):\n",
    "    gnn_model_train_eval(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 19, 20, 21]\n",
      "[0.7208499347578388, 0.7138786046235402, 0.7260651991530943, 0.7125074419948373]\n"
     ]
    }
   ],
   "source": [
    "print(output_dims)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "watch loss in train function\n",
    "print(gnn)\n",
    "for i in range(len( x_display_epochs )):\n",
    "    print('epoch {:4d},full_loss:{:6f}'.format( x_display_epochs[i], \n",
    "        val_full_loss_list[i] ) )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、Analyzing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on whether you use balanced loss or not, you should (probably) use different threshold values. From my experience, following are reasonable defaults:\n",
    " - for `balance_loss = True`: `thresh = 0.5`\n",
    " - for `balance_loss = False`: `thresh = 0.01`\n",
    " \n",
    "You can look at the distribution of the non-zero entries of `Z` to decide on a good value for the threshold. \n",
    "I guess it makes sense to select a value that lies in the leftmost \"valley\" of histogram below. \n",
    "You can also look at the unsupervised metrics in the next section of this notebook to make an informed choice.\n",
    "\n",
    "Note that all of these are just speculations based on the behavior that I observed for a handful of datasets, YMMV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the adjacency matrix sorted by the communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# 损失函数随迭代次数变化\n",
    "plt.plot(x_train_epochs, train_loss_list, label='train_loss_list')\n",
    "plt.title('training loss value towards epochs')\n",
    "plt.show()\n",
    "\n",
    "plt.title('validating value towards epochs')\n",
    "plt.plot(x_display_epochs, val_full_loss_list, label='val_full_loss_list')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(Z[Z > 0].cpu().detach().numpy(), 100);\n",
    "\n",
    "plt.figure(figsize=[12, 12])\n",
    "o = np.argsort(Z_pred)\n",
    "nocd.utils.plot_sparse_clustered_adjacency(A, n_clusters, Z_pred, o, markersize=0.05)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Sizes of detected communities\n",
    "print(Z_pred.sum(0))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantify quality of the communities based on unsupervised metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics:\n",
    "* **Coverage**: what percentage of the edges is explained by at least one community? (i.e. if $(u, v)$ is an edge, both nodes share at least one community) Higher is better.\n",
    "$$\\textrm{Coverage}(C_1, ..., C_K) = \\frac{1}{|E|}\\sum_{u, v \\in E} \\mathbb{1}[z_u^T z_v > 0]$$\n",
    "\n",
    "\n",
    "* **Density**: average density of the detected communities (weighted by community size). Higher is better.\n",
    "\n",
    "$$\\rho(C) = \\frac{\\text{# existing edges in $C$}}{\\text{# of possible edges in $C$}}$$\n",
    "\n",
    "$$\\textrm{AvgDensity}(C_1, ..., C_K) = \\frac{1}{\\sum_i |C_i|}\\sum_i \\rho(C_i) \\cdot |C_i|$$\n",
    "\n",
    "\n",
    "* **Conductance**: average conductance of the detected communities (weighted by community size). Lower is better.\n",
    "\n",
    "$$\\textrm{outside}(C) = \\sum_{u \\in C, v \\notin C} A_{uv}$$\n",
    "\n",
    "$$\\textrm{inside}(C) = \\sum_{u \\in C, v \\in C, v \\ne u} A_{uv}$$\n",
    "\n",
    "$$\\textrm{Conductance}(C) = \\frac{\\textrm{outside}(C)}{\\textrm{inside}(C) + \\textrm{outside}(C)}$$\n",
    "\n",
    "$$\\textrm{AvgConductance}(C_1, ..., C_K) = \\frac{1}{\\sum_i |C_i|}\\sum_i \\textrm{Conductance}(C_i) \\cdot |C_i|$$\n",
    "\n",
    "\n",
    "* **Clustering coefficient**: average clustering coefficient of the detected communities (weighted by community size). Higher is better.\n",
    "\n",
    "$$\\textrm{ClustCoef}(C) = \\frac{\\text{# existing triangles in $C$}}{\\text{# of possible triangles in $C$}}$$\n",
    "\n",
    "$$\\textrm{AvgClustCoef}(C_1, ..., C_K) = \\frac{1}{\\sum_i |C_i|}\\sum_i \\textrm{ClustCoef}(C_i) \\cdot |C_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering coefficient & density of the entire graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "density_baseline = A.nnz / (N**2 - N)\n",
    "num_triangles = (A @ A @ A).diagonal().sum() / 6\n",
    "num_possible_triangles = (N - 2) * (N - 1) * N / 6\n",
    "clust_coef_baseline = num_triangles / num_possible_triangles\n",
    "print(f'Background (over the entire graph):\\n'\n",
    "      f' - density    = {density_baseline:.3e}\\n'\n",
    "      f' - clust_coef = {clust_coef_baseline:.3e}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "### Ground truth communities & Predicted communities\n",
    "metrics = nocd.metrics.evaluate_unsupervised(Z_gt, A)\n",
    "print(f\"Ground truth communities:\\n\"\n",
    "      f\" - coverage    = {metrics['coverage']:.4f}\\n\"\n",
    "      f\" - conductance = {metrics['conductance']:.4f}\\n\"\n",
    "      f\" - density     = {metrics['density']:.3e}\\n\"\n",
    "      f\" - clust_coef  = {metrics['clustering_coef']:.3e}\")\n",
    "metrics = nocd.metrics.evaluate_unsupervised(Z_pred, A)\n",
    "print(f\"Predicted communities:\\n\"\n",
    "      f\" - coverage    = {metrics['coverage']:.4f}\\n\"\n",
    "      f\" - conductance = {metrics['conductance']:.4f}\\n\"\n",
    "      f\" - density     = {metrics['density']:.3e}\\n\"\n",
    "      f\" - clust_coef  = {metrics['clustering_coef']:.3e}\")\n",
    "The detected partition has lower conductance / higher density / higher clustering coefficient than the GT communities.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
