{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nocd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import normalize\n",
    "%matplotlib inline\n",
    "\n",
    "from utils import load_edgelist_graph,load_ordered_adjlist_graph\n",
    "from evaluation import modularity, symmetric_matrix_modularity\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、Defining and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset\n",
    " - `A` (adjacency matrix) is a `scipy.sparse.csr_matrix` of size `[N, N]`\n",
    " - `X` (attribute matrix) is a `np.ndarray` of size `[N, D]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin time:Fri Apr 23 22:48:05 2021\n"
     ]
    }
   ],
   "source": [
    "name = 'dpwk'\n",
    "\n",
    "print('begin time:{}'.format(time.asctime(time.localtime(time.time()))))    \n",
    "A = load_edgelist_graph(name)\n",
    "X = np.loadtxt( './features/'+name+'.txt', dtype=float)\n",
    "G_nx = load_ordered_adjlist_graph( './graphs/'+name+'.adjlist' )\n",
    "N, D = X.shape\n",
    "print('X shape:{} * {}'.format(N, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 7\n",
    "hidden_sizes = [512,512,1024,128]     # hidden sizes of the GNN\n",
    "weight_decay = 1e-2                   # strength of L2 regularization on GNN weights\n",
    "dropout = 0.5                         # whether to use dropout\n",
    "batch_norm = True                     # whether to use batch norm\n",
    "lr = 1e-3                             # learning rate\n",
    "max_epochs = 1000                     # number of epochs to train\n",
    "display_step = 25                     # how often to compute validation loss\n",
    "balance_loss = True                   # whether to use balanced loss\n",
    "stochastic_loss = True                # whether to use stochastic or full-batch training\n",
    "batch_size = 20000                    # batch size (only for stochastic training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select & normalize the feature matrix\n",
    "\n",
    "For some datasets where the features are very informative / correlated with the community structure it's better to use `X` as input (e.g. co-authorship networks w/ keywords as node features). Otherwise, you should try using `A` or `[A, X]` as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device( \"cuda\" if cuda else \"cpu\")\n",
    "print(\"use cuda: {}\".format( cuda))\n",
    "\n",
    "x_norm = normalize(X)  # node features\n",
    "# x_norm = normalize(A)  # adjacency matrix\n",
    "# x_norm = sp.hstack([normalize(X), normalize(A)])  # concatenate A and X\n",
    "x_norm = torch.Tensor(x_norm).cuda().to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = nocd.sampler.get_edge_sampler(A, batch_size, batch_size, num_workers=5)\n",
    "gnn = nocd.nn.GCN(x_norm.shape[1], hidden_sizes, n_clusters, batch_norm=batch_norm, dropout=dropout).cuda()\n",
    "adj_norm = gnn.normalize_adj(A)\n",
    "decoder = nocd.nn.BerpoDecoder(N, A.nnz, balance_loss=balance_loss)\n",
    "opt = torch.optim.Adam(gnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = np.inf\n",
    "validation_fn = lambda: val_loss\n",
    "early_stopping = nocd.train.NoImprovementStopping(validation_fn, patience=10)\n",
    "model_saver = nocd.train.ModelSaver(gnn)\n",
    "\n",
    "x_display_epochs, val_full_loss_list, x_train_epochs, train_loss_list = [],[],[],[]\n",
    "for epoch, batch in enumerate(sampler):\n",
    "    if epoch > max_epochs:\n",
    "        break\n",
    "    if epoch % display_step == 0:\n",
    "        with torch.no_grad():\n",
    "            gnn.eval()\n",
    "            # Compute validation loss\n",
    "            Z = F.relu(gnn(x_norm, adj_norm))\n",
    "            val_loss = decoder.loss_full(Z, A)\n",
    "            x_display_epochs.append( epoch )\n",
    "            val_full_loss_list.append( val_loss )\n",
    "            print(f'Epoch {epoch:4d}, loss.full = {val_loss:.4f} ')\n",
    "            \n",
    "            # Check if it's time for early stopping / to save the model\n",
    "            early_stopping.next_step()\n",
    "            if early_stopping.should_save():\n",
    "                model_saver.save()\n",
    "            if early_stopping.should_stop():\n",
    "                stop_epoch = max_epochs\n",
    "                print(f'Breaking due to early stopping at epoch {epoch}')\n",
    "                break\n",
    "            \n",
    "    # Training step\n",
    "    gnn.train()\n",
    "    opt.zero_grad()\n",
    "    Z = F.relu(gnn(x_norm, adj_norm))\n",
    "    ones_idx, zeros_idx = batch\n",
    "    if stochastic_loss:\n",
    "        loss = decoder.loss_batch(Z, ones_idx, zeros_idx)\n",
    "    else:\n",
    "        loss = decoder.loss_full(Z, A)\n",
    "    loss += nocd.utils.l2_reg_loss(gnn, scale=weight_decay)\n",
    "    x_train_epochs.append( epoch )\n",
    "    train_loss_list.append( loss )\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "print(gnn)\n",
    "print('train end time:{}'.format(time.asctime(time.localtime(time.time()))))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、Analyzing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on whether you use balanced loss or not, you should (probably) use different threshold values. From my experience, following are reasonable defaults:\n",
    " - for `balance_loss = True`: `thresh = 0.5`\n",
    " - for `balance_loss = False`: `thresh = 0.01`\n",
    " \n",
    "You can look at the distribution of the non-zero entries of `Z` to decide on a good value for the threshold. \n",
    "I guess it makes sense to select a value that lies in the leftmost \"valley\" of histogram below. \n",
    "You can also look at the unsupervised metrics in the next section of this notebook to make an informed choice.\n",
    "\n",
    "Note that all of these are just speculations based on the behavior that I observed for a handful of datasets, YMMV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数随迭代次数变化\n",
    "plt.plot(x_train_epochs, train_loss_list, label='train_loss_list')\n",
    "plt.title('training loss value towards epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('validating value towards epochs')\n",
    "plt.plot(x_display_epochs, val_full_loss_list, label='val_full_loss_list')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Z[Z > 0].cpu().detach().numpy(), 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the adjacency matrix sorted by the communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_pred = F.relu(gnn(x_norm, adj_norm))\n",
    "# Z_pred = Z.cpu().detach().numpy() > thresh \n",
    "Z_pred = np.argmax(Z_pred.cpu().detach().numpy(), 1)\n",
    "model_saver.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 10])\n",
    "# Z_pred = F.relu(gnn(x_norm, adj_norm)).cpu().detach().numpy()\n",
    "# z = np.argmax(Z_pred, 1)\n",
    "o = np.argsort(Z_pred)\n",
    "nocd.utils.plot_sparse_clustered_adjacency(A, n_clusters, Z_pred, o, markersize=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nFinal symmetric_matrix_modularity = {symmetric_matrix_modularity(G_nx, Z_pred):.8f}')\n",
    "print('evaluae modularity end time:{}'.format(time.asctime(time.localtime(time.time()))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sizes of detected communities\n",
    "print(Z_pred.sum(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantify quality of the communities based on unsupervised metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics:\n",
    "* **Coverage**: what percentage of the edges is explained by at least one community? (i.e. if $(u, v)$ is an edge, both nodes share at least one community) Higher is better.\n",
    "$$\\textrm{Coverage}(C_1, ..., C_K) = \\frac{1}{|E|}\\sum_{u, v \\in E} \\mathbb{1}[z_u^T z_v > 0]$$\n",
    "\n",
    "\n",
    "* **Density**: average density of the detected communities (weighted by community size). Higher is better.\n",
    "\n",
    "$$\\rho(C) = \\frac{\\text{# existing edges in $C$}}{\\text{# of possible edges in $C$}}$$\n",
    "\n",
    "$$\\textrm{AvgDensity}(C_1, ..., C_K) = \\frac{1}{\\sum_i |C_i|}\\sum_i \\rho(C_i) \\cdot |C_i|$$\n",
    "\n",
    "\n",
    "* **Conductance**: average conductance of the detected communities (weighted by community size). Lower is better.\n",
    "\n",
    "$$\\textrm{outside}(C) = \\sum_{u \\in C, v \\notin C} A_{uv}$$\n",
    "\n",
    "$$\\textrm{inside}(C) = \\sum_{u \\in C, v \\in C, v \\ne u} A_{uv}$$\n",
    "\n",
    "$$\\textrm{Conductance}(C) = \\frac{\\textrm{outside}(C)}{\\textrm{inside}(C) + \\textrm{outside}(C)}$$\n",
    "\n",
    "$$\\textrm{AvgConductance}(C_1, ..., C_K) = \\frac{1}{\\sum_i |C_i|}\\sum_i \\textrm{Conductance}(C_i) \\cdot |C_i|$$\n",
    "\n",
    "\n",
    "* **Clustering coefficient**: average clustering coefficient of the detected communities (weighted by community size). Higher is better.\n",
    "\n",
    "$$\\textrm{ClustCoef}(C) = \\frac{\\text{# existing triangles in $C$}}{\\text{# of possible triangles in $C$}}$$\n",
    "\n",
    "$$\\textrm{AvgClustCoef}(C_1, ..., C_K) = \\frac{1}{\\sum_i |C_i|}\\sum_i \\textrm{ClustCoef}(C_i) \\cdot |C_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering coefficient & density of the entire graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_baseline = A.nnz / (N**2 - N)\n",
    "num_triangles = (A @ A @ A).diagonal().sum() / 6\n",
    "num_possible_triangles = (N - 2) * (N - 1) * N / 6\n",
    "clust_coef_baseline = num_triangles / num_possible_triangles\n",
    "print(f'Background (over the entire graph):\\n'\n",
    "      f' - density    = {density_baseline:.3e}\\n'\n",
    "      f' - clust_coef = {clust_coef_baseline:.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth communities & Predicted communities\n",
    "metrics = nocd.metrics.evaluate_unsupervised(Z_gt, A)\n",
    "print(f\"Ground truth communities:\\n\"\n",
    "      f\" - coverage    = {metrics['coverage']:.4f}\\n\"\n",
    "      f\" - conductance = {metrics['conductance']:.4f}\\n\"\n",
    "      f\" - density     = {metrics['density']:.3e}\\n\"\n",
    "      f\" - clust_coef  = {metrics['clustering_coef']:.3e}\")\n",
    "metrics = nocd.metrics.evaluate_unsupervised(Z_pred, A)\n",
    "print(f\"Predicted communities:\\n\"\n",
    "      f\" - coverage    = {metrics['coverage']:.4f}\\n\"\n",
    "      f\" - conductance = {metrics['conductance']:.4f}\\n\"\n",
    "      f\" - density     = {metrics['density']:.3e}\\n\"\n",
    "      f\" - clust_coef  = {metrics['clustering_coef']:.3e}\")\n",
    "The detected partition has lower conductance / higher density / higher clustering coefficient than the GT communities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
