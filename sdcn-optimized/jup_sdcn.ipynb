{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bronze-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\n",
    "from sklearn.metrics import adjusted_rand_score as ari_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Linear\n",
    "from collections import Counter\n",
    "\n",
    "from sdcn.utils import load_data, load_graph\n",
    "from sdcn.GNN import GNNLayer\n",
    "from sdcn.evaluation import eva\n",
    "\n",
    "import time\n",
    "import networkx as nx\n",
    "from sdcn.evaluation import modularity\n",
    "from sdcn.pretrain import AE\n",
    "from myutils import load_ordered_graph\n",
    "from mnn.decoder import BerpoDecoder\n",
    "\n",
    "# torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "formed-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_norm = True       # whether to use batch norm\n",
    "max_epochs = 200        # number of epochs to train\n",
    "display_step = 100       # how often to compute validation loss\n",
    "stochastic_loss = True  # whether to use stochastic or full-batch training\n",
    "batch_size = 20000      # batch size (only for stochastic training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "uniform-agency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cuda: True\n",
      "training graph(./mygraph/dpwk_edgelist.txt) reading...\n",
      "reading dpwk.txt, please wait for about a thousand years....\n",
      "features shape 25023(nodes) * 128(columns) \n",
      "graph has 4624939(edges) \n"
     ]
    }
   ],
   "source": [
    "# cuda\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device( \"cuda\" if cuda else \"cpu\")\n",
    "print(\"use cuda: {}\".format( cuda))\n",
    "\n",
    "'''\n",
    " 'bio72' not ok, almost all in one community\n",
    "'''\n",
    "# Graph matrix (coo_matrix, csr_matrix, dense )\n",
    "name = 'dpwk'\n",
    "A, A_csr, A_dense = load_graph( name )\n",
    "A = A.cuda()\n",
    "# AttributeError: cuda not found\n",
    "# A_csr = A_csr.cuda()\n",
    "A_dense = torch.Tensor(A_dense).to(device)\n",
    "# Features\n",
    "dataset = load_data( name )\n",
    "N, K = len(dataset.x), len(dataset.x[0])\n",
    "print('features shape {}(nodes) * {}(columns) '.format(N,K))\n",
    "# Loss class\n",
    "decoder = BerpoDecoder(N, A_csr.nnz, balance_loss=False)\n",
    "print('graph has {}(edges) '.format(A_csr.nnz))\n",
    "# pretrain AutoEncoder data\n",
    "pretrain_path = './pretrain/' + name + '.pkl'\n",
    "# sdcn epochs\n",
    "epochs = 50 # 30 # 100 # 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "biblical-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning model \n",
    "class SDCN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input, n_1, n_2, n_3, n_z, n_d3, n_d2, n_d1, n_clusters, v=1):\n",
    "        super(SDCN, self).__init__()\n",
    "\n",
    "        # TODO  not separately pretrain but altogether\n",
    "        \n",
    "        # autoencoder for intra information    #  symmetric \n",
    "        self.ae = AE( args.n_input, args.n_1, args.n_2, args.n_3, \n",
    "                     args.n_z, args.n_3, args.n_2, args.n_1 )\n",
    "        self.ae.load_state_dict(torch.load( pretrain_path, map_location='cpu'))\n",
    "\n",
    "        # GCN for inter information\n",
    "        self.gnn_1 = GNNLayer(n_input, n_1)\n",
    "        self.gnn_2 = GNNLayer(n_1, n_2)\n",
    "        self.gnn_3 = GNNLayer(n_2, n_3)\n",
    "        self.gnn_4 = GNNLayer(n_3, n_z)\n",
    "        self.gnn_5 = GNNLayer(n_z, n_clusters)\n",
    "\n",
    "        # cluster layer\n",
    "        self.cluster_layer = Parameter(torch.Tensor(n_clusters, n_z))\n",
    "        torch.nn.init.xavier_normal_(self.cluster_layer.data)\n",
    "\n",
    "        # degree\n",
    "        self.v = v\n",
    "\n",
    "    def forward(self, x, A):\n",
    "        # DNN Module\n",
    "        x_bar, tra1, tra2, tra3, z = self.ae(x)\n",
    "\n",
    "        # GCN Module\n",
    "        h1 = self.gnn_1(x, A)\n",
    "        h2 = self.gnn_2(h1+tra1, A)\n",
    "        h3 = self.gnn_3(h2+tra2, A)\n",
    "        h4 = self.gnn_4(h3+tra3, A)\n",
    "        h5 = self.gnn_5(h4+z, A, active=False)\n",
    "        predict = F.softmax(h5, dim=1)\n",
    "        # z_sdcn = F.relu(h5)\n",
    "        \n",
    "        '''\n",
    "        # Dual Self-supervised Module\n",
    "        q = 1.0 / (1.0 + torch.sum(torch.pow(z.unsqueeze(1) - self.cluster_layer, 2), 2) / self.v)\n",
    "        q = q.pow((self.v + 1.0) / 2.0)\n",
    "        q = (q.t() / torch.sum(q, 1)).t()\n",
    "        '''\n",
    "       \n",
    "        return x_bar, predict, z # , z_sdcn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "parallel-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_distribution(q):\n",
    "    weight = q**2 / q.sum(0)\n",
    "    return (weight.t() / weight.sum(1)).t()\n",
    "\n",
    "\n",
    "def train_sdcn(dataset, args):\n",
    "    # initial dimensions 500,500,2000,10,2000,500,500\n",
    "    model = SDCN(   n_input=args.n_input, n_1 = args.n_1, n_2 = args.n_2, n_3 = args.n_3, \n",
    "                    n_z=args.n_z, n_d3 = args.n_3, n_d2 = args.n_2, n_d1 = args.n_1, \n",
    "                    n_clusters=args.n_clusters, v=1.0).to(device)\n",
    "    print(model)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # X\n",
    "    data = torch.Tensor(dataset.x).to(device)\n",
    "    \n",
    "    '''\n",
    "    # TODO only AutoEncoder result\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, _, _, _, z = model.ae(data)\n",
    "    kmeans = KMeans(n_clusters=args.n_clusters, n_init=20)\n",
    "    y_pred = kmeans.fit_predict(z.data.cpu().numpy())\n",
    "    y_pred_last = y_pred\n",
    "    model.cluster_layer.data = torch.tensor(kmeans.cluster_centers_).to(device)\n",
    "    # eva(y, y_pred, 'pae')\n",
    "    '''\n",
    "    \n",
    "    # training...\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        x_bar, pred, z  = model(data, A)\n",
    "        \n",
    "        '''\n",
    "        # my initial naive loss\n",
    "        re_loss = F.mse_loss(x_bar, data)\n",
    "        # 社区发现重构边损失\n",
    "        A_rec = torch.mm( z_sdcn, z_sdcn.T ).cuda()     \n",
    "        cd_loss = F.mse_loss( A_rec, A_dense )\n",
    "        # 72维features重构、25023*25023图社区划分结果重构损失加权\n",
    "        loss =  0.001 * cd_loss + re_loss          \n",
    "        '''\n",
    "        \n",
    "        # predict result\n",
    "        res = pred.data.cpu().numpy().argmax(1)   #Z\n",
    "        # BernoulliDecoder loss\n",
    "        loss = decoder.loss_full( pred, A_csr )\n",
    "        print('loss of epoch {}:{} '.format(epoch,loss))\n",
    "        \n",
    "        \n",
    "        # last modularity evaluation \n",
    "        if epoch == epochs-1:    \n",
    "            # G = nx.read_adjlist( './mygraph/bio30_ps.adjlist' )    # my graph\n",
    "            graph_path = ''\n",
    "            if  name == 'bio72':\n",
    "                graph_path = './mygraph/bio30_ps.adjlist'\n",
    "            elif args.name in ['dpwk','line','lle','n2v'] :\n",
    "                graph_path = './od2graphs/' +  name + '.adjlist'\n",
    "            print('evaluating graph({}) reading...'.format(graph_path))\n",
    "            G = load_ordered_graph( graph_path )\n",
    "\n",
    "            pred_dic = {}\n",
    "            for idx,pred_label in enumerate(res):\n",
    "                pred_dic[idx] = pred_label\n",
    "            pred_modul = modularity(G, pred_dic)\n",
    "            print('epoch {}  modularity {:.4f}'.format(epoch,pred_modul))\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adjacent-landing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin time:Sun Apr 18 16:01:18 2021\n",
      "Namespace(data_path='./mydata/bio72.csv', k=3, lr=0.001, n_1=500, n_2=500, n_3=2000, n_clusters=8, n_input=128, n_z=10, name='dpwk', pretrain_path='./pretrain/bio72.pkl')\n",
      "SDCN(\n",
      "  (ae): AE(\n",
      "    (enc_1): Linear(in_features=128, out_features=500, bias=True)\n",
      "    (enc_2): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (enc_3): Linear(in_features=500, out_features=2000, bias=True)\n",
      "    (z_layer): Linear(in_features=2000, out_features=10, bias=True)\n",
      "    (dec_3): Linear(in_features=10, out_features=2000, bias=True)\n",
      "    (dec_2): Linear(in_features=2000, out_features=500, bias=True)\n",
      "    (dec_1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (x_bar_layer): Linear(in_features=500, out_features=128, bias=True)\n",
      "  )\n",
      "  (gnn_1): GNNLayer()\n",
      "  (gnn_2): GNNLayer()\n",
      "  (gnn_3): GNNLayer()\n",
      "  (gnn_4): GNNLayer()\n",
      "  (gnn_5): GNNLayer()\n",
      ")\n",
      "loss of epoch 0:0.14422288537025452 \n",
      "loss of epoch 1:0.19342853128910065 \n",
      "loss of epoch 2:0.14399345219135284 \n",
      "loss of epoch 3:0.14506198465824127 \n",
      "loss of epoch 4:0.14463406801223755 \n",
      "loss of epoch 5:0.1437518298625946 \n",
      "loss of epoch 6:0.14320038259029388 \n",
      "loss of epoch 7:0.14281076192855835 \n",
      "loss of epoch 8:0.14250583946704865 \n",
      "loss of epoch 9:0.14219947159290314 \n",
      "loss of epoch 10:0.1418326497077942 \n",
      "loss of epoch 11:0.14144498109817505 \n",
      "loss of epoch 12:0.14110513031482697 \n",
      "loss of epoch 13:0.14081884920597076 \n",
      "loss of epoch 14:0.14054647088050842 \n",
      "loss of epoch 15:0.1403341144323349 \n",
      "loss of epoch 16:0.14024999737739563 \n",
      "loss of epoch 17:0.14019030332565308 \n",
      "loss of epoch 18:0.14007534086704254 \n",
      "loss of epoch 19:0.13995666801929474 \n",
      "loss of epoch 20:0.13988512754440308 \n",
      "loss of epoch 21:0.1398475617170334 \n",
      "loss of epoch 22:0.13979877531528473 \n",
      "loss of epoch 23:0.1397406905889511 \n",
      "loss of epoch 24:0.13968567550182343 \n",
      "loss of epoch 25:0.1396387815475464 \n",
      "loss of epoch 26:0.13960285484790802 \n",
      "loss of epoch 27:0.13957908749580383 \n",
      "loss of epoch 28:0.13956691324710846 \n",
      "loss of epoch 29:0.1395636349916458 \n",
      "loss of epoch 30:0.13956500589847565 \n",
      "loss of epoch 31:0.139566570520401 \n",
      "loss of epoch 32:0.13956566154956818 \n",
      "loss of epoch 33:0.13956116139888763 \n",
      "loss of epoch 34:0.1395534873008728 \n",
      "loss of epoch 35:0.13954387605190277 \n",
      "loss of epoch 36:0.13953374326229095 \n",
      "loss of epoch 37:0.13952437043190002 \n",
      "loss of epoch 38:0.13951684534549713 \n",
      "loss of epoch 39:0.13951168954372406 \n",
      "loss of epoch 40:0.13950911164283752 \n",
      "loss of epoch 41:0.1395084410905838 \n",
      "loss of epoch 42:0.13950850069522858 \n",
      "loss of epoch 43:0.13950811326503754 \n",
      "loss of epoch 44:0.13950695097446442 \n",
      "loss of epoch 45:0.13950519263744354 \n",
      "loss of epoch 46:0.13950324058532715 \n",
      "loss of epoch 47:0.1395014226436615 \n",
      "loss of epoch 48:0.13949990272521973 \n",
      "loss of epoch 49:0.1394987404346466 \n",
      "evaluating graph(./od2graphs/dpwk.adjlist) reading...\n",
      "node community distribute:{0: 2850, 1: 3169, 2: 2385, 3: 3436, 4: 2265, 5: 953, 6: 9227, 7: 738}\n",
      "modularity running node:10 i:10,node_i:10\n",
      "modularity running node:25022now Q(real 2mQ):780199.2009155304\n",
      "edgs:2324981\n",
      "total com_i==com_j count:127379989\n",
      "epoch 49  modularity 0.1678\n",
      "end time:Sun Apr 18 16:16:02 2021\n"
     ]
    }
   ],
   "source": [
    "# 矩阵25023 * 25023 内存不够，彻底失败\n",
    "if __name__ == \"__main__\":\n",
    "    print('begin time:{}'.format(time.asctime(time.localtime(time.time()))))    # time \n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='train',\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--name', type=str, default= name )\n",
    "    parser.add_argument('--k', type=int, default=3)\n",
    "    parser.add_argument('--lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--pretrain_path', type=str, default='./pretrain/bio72.pkl')\n",
    "    parser.add_argument('--data_path', type=str, default = './mydata/bio72.csv')    # redundant......\n",
    "    \n",
    "    parser.add_argument('--n_input', default=72, type=int)\n",
    "    parser.add_argument('--n_1', default=500, type=int)             # dimensions \n",
    "    parser.add_argument('--n_2', default=500, type=int)             # n_in  ->  n_1  ->  n_2  ->  n_3  ->  n_z\n",
    "    parser.add_argument('--n_3', default=2000, type=int)            # n_in  <-  d_1  <-  d_2  <-  d_3  <-  n_z\n",
    "    parser.add_argument('--n_z', default=10, type=int)\n",
    "    parser.add_argument('--n_clusters', default=8, type=int)\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "\n",
    "    if args.name == 'bio72':\n",
    "        args.n_input = 72\n",
    "        args.pretrain_path = './pretrain/bio72.pkl'\n",
    "        args.data_path = './mydata/bio72.csv'\n",
    "    \n",
    "    if args.name in ['dpwk','line','lle','n2v'] :\n",
    "        args.lr = 1e-3\n",
    "        args.n_input = 128\n",
    "        \n",
    "    if args.name == 'biomat':      # totally failed\n",
    "        args.n_input = 25023\n",
    "        args.pretrain_path = './pretrain/biomat.pkl'\n",
    "        args.data_path = './mydata/biomat.txt'\n",
    "    \n",
    "    \n",
    "    print(args)\n",
    "    train_sdcn(dataset, args)\n",
    "    print('end time:{}'.format(time.asctime(time.localtime(time.time()))))    # time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-disposal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-consultation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-update",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
